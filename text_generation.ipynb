{"cells":[{"cell_type":"markdown","metadata":{"id":"t09eeeR5prIJ"},"source":["##### Copyright 2019 The TensorFlow Authors.\n","##### NOTICE: heavily modified by author of this repository"]},{"cell_type":"markdown","metadata":{"id":"ovpZyIhNIgoq"},"source":["# Text generation with an RNN"]},{"cell_type":"markdown","metadata":{"id":"srXC6pLGLwS6"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{},"source":["this is a test \"program\" to make sure python works "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1705649312784,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"E3iSOMjuQT0l","outputId":"1b34f642-2e7f-42c8-c8c6-3a01f6d44557"},"outputs":[],"source":["\"helo worldlrdl22\""]},{"cell_type":"markdown","metadata":{"id":"WGyKZj3bzf9p"},"source":["### Import TensorFlow and other libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3452,"status":"ok","timestamp":1705649374601,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"yG_n40gFzf9s"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","\n","import numpy as np\n","import time"]},{"cell_type":"markdown","metadata":{"id":"EHDoRoc5PKWz"},"source":["this imports the data, if you arent using google colab change the `path_to_file` variable to point wherever your comments dataset is "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17901,"status":"ok","timestamp":1704874198709,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"pD_55cOxLkAb","outputId":"a20377cd-4d38-4060-ba63-3d3853ab3056"},"outputs":[],"source":["from google.colab import drive # remove this if you arent using google colab\n","drive.mount(\"/content/drive\") # remove this if you arent using google colab\n","path_to_file = \"/content/drive/MyDrive/gdpt/comments.txt\" \n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"]},{"cell_type":"markdown","metadata":{"id":"UHjdCjDuSvX_"},"source":["this cell is optional, you can skip it if you want to"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":620,"status":"ok","timestamp":1704874205155,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"aavnuByVymwK","outputId":"530388b8-5c68-4ac8-891e-ee4fbb229978"},"outputs":[],"source":["print(f'Length of text: {len(text)} characters')\n","print(text[:250])\n","\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique characters')"]},{"cell_type":"markdown","metadata":{},"source":["this cell defines important variables and functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GMlCe3qzaL9"},"outputs":[],"source":["ids_from_chars = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab), mask_token=None)\n","chars_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n","def text_from_ids(ids):\n","  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n","all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"]},{"cell_type":"markdown","metadata":{},"source":["you can change the `seq_length` variable if you like to experiment a lil bit "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-G2oaTxy6km"},"outputs":[],"source":["seq_length = 100 \n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"]},{"cell_type":"markdown","metadata":{},"source":["### this step is important\n","change the batch_size depending on your model size, see more info in the code comments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1704874248035,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"p2pGotuNzf-S","outputId":"19d25303-16bf-4d15-f050-56a3a28b045b"},"outputs":[],"source":["# Batch size\n","# if you have 1-3 levels scraped use 64\n","# if you have more 128 is prefered\n","# you are free to experiment here btw\n","BATCH_SIZE = 64\n","\n","# i dont recommend changing this\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wj8HQ2w8z4iO"},"outputs":[],"source":["vocab_size = len(ids_from_chars.get_vocabulary())\n","\n","# better keep this as it is\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","class MyModel(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    x, states = self.gru(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x\n","    \n","model = MyModel(\n","vocab_size=vocab_size,\n","embedding_dim=embedding_dim,\n","rnn_units=rnn_units)"]},{"cell_type":"markdown","metadata":{"id":"-ubPo0_9Prjb"},"source":["\"run\" the model to make sure its good to go"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3630,"status":"ok","timestamp":1704874264998,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"C-_70kKAPrPU","outputId":"b589a3b2-a83c-4a60-c3ac-667323c707f7"},"outputs":[],"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","    \n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["this cell is optional"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4V4MfFg0RQJg"},"outputs":[],"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","\n","sampled_indices\n","\n","print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n","print()\n","print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"]},{"cell_type":"markdown","metadata":{"id":"LJL0Q0YPY6Ee"},"source":["### training"]},{"cell_type":"markdown","metadata":{},"source":["loss is how much the model sure of its answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOeWdgxNFDXq"},"outputs":[],"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1704874280419,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"4HrXTACTdzY-","outputId":"ab098899-827c-4603-f07c-abf23cb25a27"},"outputs":[],"source":["example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"Mean loss:        \", example_batch_mean_loss)\n","\n","tf.exp(example_batch_mean_loss).numpy() # this should return value similar to vocab_size higher; if not the model is badly initialized\n","\n","# config the training procedure\n","model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"C6XBUUavgF56"},"source":["this cell configures where the checkpoints will be saved"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W6fWTriUZP-n"},"outputs":[],"source":["checkpoint_dir = '/content/training_checkpoints' # if you arent using google colab replace this with './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"]},{"cell_type":"markdown","metadata":{"id":"IxdOA-rgyGvs"},"source":["the `EPOCHS` variable is responsible for how many times the ai goes through the dataset "]},{"cell_type":"markdown","metadata":{},"source":["right now is also a good time to mention that if youre using google colab change the runtime to gpu for faster training (optional, but recommended) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yGBE2zxMMHs"},"outputs":[],"source":["EPOCHS = 10 # values between 10-30 should be good, the higher the better the model will be but also training will be slower\n","# if you put too much epochs it will overfit and the model will be worse quality"]},{"cell_type":"markdown","metadata":{},"source":["the cell below starts training, if you interrupt it at any point the model may become corrupted"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UK-hmKjYVoll","outputId":"09f8e3e2-3274-4f0d-a8e2-4e6d0038a5c4"},"outputs":[],"source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"]},{"cell_type":"markdown","metadata":{"id":"kKkD5M6eoSiN"},"source":["### wait until the model trains before continuing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSBU1tHmlUSs"},"outputs":[],"source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    predicted_logits = predicted_logits + self.prediction_mask\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","    return predicted_chars, states\n","\n","one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"]},{"cell_type":"markdown","metadata":{"id":"p9yDoa0G3IgQ"},"source":["this cell actually runs the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3234,"status":"ok","timestamp":1704826336350,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"ST7PSyk9t1mT","outputId":"c1c75e01-c9a5-45d7-a21e-c1c5ac1d0a1c"},"outputs":[],"source":["start = time.time()\n","states = None\n","next_char = tf.constant(['\\n'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"]},{"cell_type":"markdown","metadata":{"id":"_OfbI4aULmuj"},"source":["you can pass multiple strings to the `next_char` variable, if you do it will generate text faster "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4321,"status":"ok","timestamp":1704826340661,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"ZkLu7Y8UCMT7","outputId":"d9ac6b9d-817f-4db5-94d3-8485955f88a8"},"outputs":[],"source":["start = time.time()\n","states = None\n","next_char = tf.constant(['\\n', '\\n', '\\n', '\\n', '\\n'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result, '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"]},{"cell_type":"markdown","metadata":{"id":"UlUQzwu6EXam"},"source":["save the model so you can use it anytime without having to run the training again"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4078,"status":"ok","timestamp":1704826445727,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"3Grk32H_CzsC","outputId":"48e6d2e5-afba-42d4-9753-df75df7fde30"},"outputs":[],"source":["tf.saved_model.save(one_step_model, 'one_step')\n","one_step_reloaded = tf.saved_model.load('one_step')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1579,"status":"ok","timestamp":1704826347639,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"_Z9bb_wX6Uuu","outputId":"6943f4ab-2518-481b-8c6e-b45a7405b9ef"},"outputs":[],"source":["states = None\n","next_char = tf.constant(['\\n'])\n","result = [next_char]\n","\n","for n in range(500):\n","  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"]},{"cell_type":"markdown","metadata":{"id":"yl9Q52RADTzp"},"source":["put the model into an archive for safekeeping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":3681,"status":"ok","timestamp":1704826457201,"user":{"displayName":"Ignatik Klokov","userId":"01052346871717812908"},"user_tz":-120},"id":"lB4PajANDH4Y","outputId":"65c5454a-2739-4e7a-8cc2-6f0d199f614e"},"outputs":[],"source":["import shutil\n","shutil.make_archive(\"one_step\", 'zip', \"/content/one_step\") # if not on google colab change /content/one_step to ./one_step"]},{"cell_type":"markdown","metadata":{},"source":["## congrats, you now should have a working text model ready for use\n","\n","the end btw"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
